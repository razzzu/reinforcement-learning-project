{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "Ygv2OXM-VKiS",
    "outputId": "454b7970-0b3d-416e-86cc-8f32117f27bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "colab_type": "code",
    "id": "p-ZucKRlV31F",
    "outputId": "88301c70-5e37-430c-c4be-29a1db1ec64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
      "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.3.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.17.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
      "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.2)\n",
      "Requirement already satisfied: tensorflow-gpu==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.10.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.6)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.17.4)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (41.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (3.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl\n",
    "!pip install keras==2.2.4\n",
    "!pip install tensorflow-gpu==1.13.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "MGq9Tc58V6Kz",
    "outputId": "c4e9521b-d8d0-4de1-a4bb-6c7b3107d8eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "import atari_py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "\n",
    "#print(atari_py.list_games())\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQ9Zw92JV8bd"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('float32')/255 \n",
    "\n",
    "    #def process_reward(self, reward):\n",
    "     #   return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD0PFtvGV-Wj"
   },
   "outputs": [],
   "source": [
    "#game = 'BeamRider-v0' #9\n",
    "#game = 'Breakout-v0' #4\n",
    "#game = 'Solaris-v0' #18\n",
    "#game = 'DemonAttack-v0' #6\n",
    "#game = 'AirRaid-v0' #6\n",
    "\n",
    "game1 = 'AirRaid-v0' #6\n",
    "game2 = 'Centipede-v0' #18\n",
    "\n",
    "env1 = gym.make(game1)\n",
    "np.random.seed(123)\n",
    "env1.seed(123)\n",
    "env1_nb_actions = env1.action_space.n\n",
    "\n",
    "env2 = gym.make(game2)\n",
    "np.random.seed(123)\n",
    "env2.seed(123)\n",
    "env2_nb_actions = env2.action_space.n\n",
    "\n",
    "total_actions = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WGxWiFw1WBIE",
    "outputId": "f4f9d8a5-7e2d-44f9-f47a-bcbaa062b344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "shared_layer (Dense)         (None, 18)                9234      \n",
      "_________________________________________________________________\n",
      "env1_output (Dense)          (None, 6)                 114       \n",
      "=================================================================\n",
      "Total params: 1,693,476\n",
      "Trainable params: 1,693,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "shared_layer (Dense)         (None, 18)                9234      \n",
      "_________________________________________________________________\n",
      "env2_output (Dense)          (None, 18)                342       \n",
      "=================================================================\n",
      "Total params: 1,693,704\n",
      "Trainable params: 1,693,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "\n",
    "# CNN for environment 1\n",
    "env1_input_layer = Input(shape=input_shape)\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    env1_permute = Permute((2, 3, 1))(env1_input_layer)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    env1_permute = Permute((1, 2, 3))(env1_input_layer)\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering for environment 1.')\n",
    "\n",
    "env1_Conv_1 = Convolution2D(32, (8, 8), strides=(4, 4), activation = 'relu')(env1_permute)\n",
    "env1_Conv_2 = Convolution2D(64, (4, 4), strides=(2, 2), activation = 'relu')(env1_Conv_1)\n",
    "env1_Conv_3 = Convolution2D(64, (3, 3), strides=(1, 1), activation = 'relu')(env1_Conv_2)\n",
    "env1_flatten = Flatten()(env1_Conv_3)\n",
    "env1_dense = Dense(512, activation = 'relu')(env1_flatten)\n",
    "\n",
    "\n",
    "# CNN for environment 2\n",
    "env2_input_layer = Input(shape=input_shape)\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    env2_permute = Permute((2, 3, 1))(env2_input_layer)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    env2_permute = Permute((1, 2, 3))(env2_input_layer)\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering for environment 2.')\n",
    "    \n",
    "env2_Conv_1 = Convolution2D(32, (8, 8), strides=(4, 4), activation = 'relu')(env2_permute)\n",
    "env2_Conv_2 = Convolution2D(64, (4, 4), strides=(2, 2), activation = 'relu')(env2_Conv_1)\n",
    "env2_Conv_3 = Convolution2D(64, (3, 3), strides=(1, 1), activation = 'relu')(env2_Conv_2)\n",
    "env2_flatten = Flatten()(env2_Conv_3)\n",
    "env2_dense = Dense(512, activation = 'relu')(env2_flatten)\n",
    "\n",
    "shared_layer = Dense(total_actions, activation = 'linear', name = 'shared_layer')    #Shared layer\n",
    "\n",
    "env1_output = Dense(env1_nb_actions, activation = 'linear', name = 'env1_output')(shared_layer(env1_dense))\n",
    "env2_output =  Dense(env2_nb_actions, activation = 'linear', name = 'env2_output')(shared_layer(env2_dense))\n",
    "\n",
    "\n",
    "model1 = Model(inputs = [env1_input_layer], outputs = [env1_output]) \n",
    "model2 =  Model(inputs = [env2_input_layer], outputs = [env2_output])\n",
    "\n",
    "\n",
    "print(model1.summary())\n",
    "plot_model(model1)\n",
    "print(model2.summary())\n",
    "plot_model(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zxrYJtNWDXl"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn1 = DQNAgent(model=model1, nb_actions=env1_nb_actions, policy=policy, memory=memory, processor=processor, nb_steps_warmup=50000, target_model_update=10000,train_interval=4)\n",
    "dqn1.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vwl-a6KZWHgQ"
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback as KerasCallback\n",
    "# class save_at_episode(KerasCallback):\n",
    "#     def on_episode_end(self, episode, logs={}):\n",
    "#         self.model.save_weights('drive/My Drive/Colab Notebooks/Game Data/Breakout_weights_v1_{}'.format(episode), overwrite=True)\n",
    "#         #print(\" saved episode:{}\".format(episode))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c2fU46AYWKDZ",
    "outputId": "c0c365fa-07b5-40ad-d0d3-1081e36b9e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 30s 3ms/step - reward: 0.7325\n",
      "13 episodes - episode_reward: 534.615 [225.000, 950.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 0.7125\n",
      "13 episodes - episode_reward: 542.308 [75.000, 1250.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 0.6725\n",
      "17 episodes - episode_reward: 407.353 [100.000, 975.000] - ale.lives: 0.998\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 0.7075\n",
      "12 episodes - episode_reward: 610.417 [150.000, 1200.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 25s 3ms/step - reward: 0.7500\n",
      "13 episodes - episode_reward: 501.923 [250.000, 950.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "    1/10000 [..............................] - ETA: 1:27 - reward: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 0.8050\n",
      "14 episodes - episode_reward: 637.500 [225.000, 1500.000] - loss: 24.968 - mean_absolute_error: 0.700 - mean_q: 0.901 - mean_eps: 0.951 - ale.lives: 0.999\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.6575\n",
      "13 episodes - episode_reward: 500.000 [150.000, 1150.000] - loss: 26.599 - mean_absolute_error: 1.288 - mean_q: 1.664 - mean_eps: 0.942 - ale.lives: 0.999\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8000\n",
      "10 episodes - episode_reward: 745.000 [200.000, 1750.000] - loss: 26.856 - mean_absolute_error: 2.225 - mean_q: 2.981 - mean_eps: 0.933 - ale.lives: 0.999\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.7625\n",
      "9 episodes - episode_reward: 908.333 [475.000, 1775.000] - loss: 28.161 - mean_absolute_error: 2.698 - mean_q: 3.703 - mean_eps: 0.924 - ale.lives: 0.999\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7875\n",
      "15 episodes - episode_reward: 521.667 [100.000, 1325.000] - loss: 26.581 - mean_absolute_error: 3.227 - mean_q: 4.348 - mean_eps: 0.915 - ale.lives: 0.999\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.8800\n",
      "12 episodes - episode_reward: 727.083 [250.000, 1500.000] - loss: 26.306 - mean_absolute_error: 4.128 - mean_q: 5.527 - mean_eps: 0.906 - ale.lives: 0.999\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7700\n",
      "13 episodes - episode_reward: 584.615 [150.000, 1525.000] - loss: 29.308 - mean_absolute_error: 4.748 - mean_q: 6.344 - mean_eps: 0.897 - ale.lives: 0.999\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7950\n",
      "16 episodes - episode_reward: 510.938 [100.000, 1825.000] - loss: 30.962 - mean_absolute_error: 6.061 - mean_q: 8.012 - mean_eps: 0.888 - ale.lives: 0.998\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.8150\n",
      "15 episodes - episode_reward: 523.333 [125.000, 975.000] - loss: 31.720 - mean_absolute_error: 7.470 - mean_q: 9.615 - mean_eps: 0.879 - ale.lives: 0.999\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.6975\n",
      "17 episodes - episode_reward: 438.235 [100.000, 1200.000] - loss: 33.433 - mean_absolute_error: 8.826 - mean_q: 11.375 - mean_eps: 0.870 - ale.lives: 0.998\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7950\n",
      "15 episodes - episode_reward: 511.667 [50.000, 975.000] - loss: 31.973 - mean_absolute_error: 9.944 - mean_q: 12.672 - mean_eps: 0.861 - ale.lives: 0.999\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7150\n",
      "11 episodes - episode_reward: 572.727 [125.000, 1300.000] - loss: 34.340 - mean_absolute_error: 11.188 - mean_q: 14.269 - mean_eps: 0.852 - ale.lives: 0.999\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7625\n",
      "15 episodes - episode_reward: 571.667 [200.000, 1200.000] - loss: 33.776 - mean_absolute_error: 12.402 - mean_q: 15.778 - mean_eps: 0.843 - ale.lives: 0.999\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.8425\n",
      "13 episodes - episode_reward: 575.000 [100.000, 1575.000] - loss: 34.195 - mean_absolute_error: 14.033 - mean_q: 17.624 - mean_eps: 0.834 - ale.lives: 0.999\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.9200\n",
      "10 episodes - episode_reward: 880.000 [400.000, 1850.000] - loss: 38.101 - mean_absolute_error: 15.812 - mean_q: 19.753 - mean_eps: 0.825 - ale.lives: 0.999\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.7700\n",
      "10 episodes - episode_reward: 882.500 [300.000, 1725.000] - loss: 38.419 - mean_absolute_error: 17.464 - mean_q: 21.737 - mean_eps: 0.816 - ale.lives: 0.999\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7875\n",
      "16 episodes - episode_reward: 517.188 [200.000, 1800.000] - loss: 39.168 - mean_absolute_error: 18.516 - mean_q: 22.943 - mean_eps: 0.807 - ale.lives: 0.998\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.8325\n",
      "10 episodes - episode_reward: 710.000 [75.000, 1975.000] - loss: 38.230 - mean_absolute_error: 20.555 - mean_q: 25.439 - mean_eps: 0.798 - ale.lives: 0.999\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8800\n",
      "14 episodes - episode_reward: 675.000 [275.000, 1625.000] - loss: 37.835 - mean_absolute_error: 21.361 - mean_q: 26.449 - mean_eps: 0.789 - ale.lives: 0.999\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.9550\n",
      "13 episodes - episode_reward: 761.538 [150.000, 1775.000] - loss: 37.878 - mean_absolute_error: 22.394 - mean_q: 27.666 - mean_eps: 0.780 - ale.lives: 0.999\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.7825\n",
      "12 episodes - episode_reward: 666.667 [225.000, 1800.000] - loss: 38.658 - mean_absolute_error: 23.719 - mean_q: 29.279 - mean_eps: 0.771 - ale.lives: 0.999\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8850\n",
      "12 episodes - episode_reward: 704.167 [300.000, 1325.000] - loss: 37.843 - mean_absolute_error: 24.371 - mean_q: 30.047 - mean_eps: 0.762 - ale.lives: 0.999\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7725\n",
      "11 episodes - episode_reward: 711.364 [175.000, 1700.000] - loss: 37.627 - mean_absolute_error: 26.349 - mean_q: 32.497 - mean_eps: 0.753 - ale.lives: 0.999\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.7575\n",
      "21 episodes - episode_reward: 375.000 [50.000, 975.000] - loss: 39.313 - mean_absolute_error: 27.268 - mean_q: 33.580 - mean_eps: 0.744 - ale.lives: 0.998\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8625\n",
      "done, took 1727.817 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn1.fit(env1, nb_steps=300000, visualize=False, verbose=1) #callbacks=[save_at_episode()]\n",
    "dqn1.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_v2',overwrite=True)\n",
    "#'/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "ETX3ntuYWL2A",
    "outputId": "3f625163-39f2-40d4-f911-8ce6f00db4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 1175.000, steps: 3509\n",
      "Episode 2: reward: 375.000, steps: 1998\n",
      "Episode 3: reward: 1775.000, steps: 4657\n",
      "Episode 4: reward: 1350.000, steps: 3566\n",
      "Episode 5: reward: 1000.000, steps: 3615\n",
      "Episode 6: reward: 600.000, steps: 3211\n",
      "Episode 7: reward: 1375.000, steps: 3270\n",
      "Episode 8: reward: 450.000, steps: 2267\n",
      "Episode 9: reward: 775.000, steps: 2635\n",
      "Episode 10: reward: 650.000, steps: 2742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9723ae0710>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn1.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_v2')\n",
    "dqn1.test(env1, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wcCV4bWa06jk",
    "outputId": "5a3a2736-8c39-48f1-ace8-0e86c15fef0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "shared_weights, shared_bias = model1.get_layer('shared_layer').weights\n",
    "sns.distplot(K.eval(shared_weights).ravel())\n",
    "plt.savefig('/content/drive/My Drive/Colab Notebooks/Game Data/shared_weights_before_v2.jpg')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uxnc2Z7v4AqS"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn2 = DQNAgent(model=model2, nb_actions=env2_nb_actions, policy=policy, memory=memory, processor=processor, nb_steps_warmup=50000, target_model_update=10000,train_interval=4)\n",
    "dqn2.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LdllTK414OS4",
    "outputId": "e6a2d6de-ebf4-4a59-c7ac-5fa50a55667e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 2.5046\n",
      "9 episodes - episode_reward: 2729.333 [918.000, 3906.000] - ale.lives: 2.022\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 2.5588\n",
      "11 episodes - episode_reward: 2248.091 [745.000, 8097.000] - ale.lives: 2.006\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 2.9687\n",
      "9 episodes - episode_reward: 3219.556 [770.000, 6850.000] - ale.lives: 2.021\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 2.2356\n",
      "11 episodes - episode_reward: 2208.000 [680.000, 3485.000] - ale.lives: 2.048\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 2.2771\n",
      "9 episodes - episode_reward: 2477.889 [485.000, 4944.000] - ale.lives: 1.943\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 2.0809\n",
      "12 episodes - episode_reward: 1773.250 [1030.000, 3184.000] - loss: 475.433 - mean_absolute_error: 2.204 - mean_q: 4.459 - mean_eps: 0.951 - ale.lives: 2.013\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.0199\n",
      "8 episodes - episode_reward: 2416.375 [950.000, 4427.000] - loss: 450.476 - mean_absolute_error: 6.512 - mean_q: 9.628 - mean_eps: 0.942 - ale.lives: 2.001\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.7811\n",
      "10 episodes - episode_reward: 2616.000 [422.000, 6134.000] - loss: 451.654 - mean_absolute_error: 9.651 - mean_q: 13.224 - mean_eps: 0.933 - ale.lives: 1.927\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 3.0108\n",
      "10 episodes - episode_reward: 3261.700 [883.000, 6065.000] - loss: 569.655 - mean_absolute_error: 12.389 - mean_q: 16.854 - mean_eps: 0.924 - ale.lives: 1.899\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.2803\n",
      "12 episodes - episode_reward: 1909.250 [492.000, 2885.000] - loss: 531.999 - mean_absolute_error: 13.950 - mean_q: 19.774 - mean_eps: 0.915 - ale.lives: 2.049\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.6216\n",
      "12 episodes - episode_reward: 2186.417 [1112.000, 3791.000] - loss: 462.038 - mean_absolute_error: 15.309 - mean_q: 20.384 - mean_eps: 0.906 - ale.lives: 2.069\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.5920\n",
      "9 episodes - episode_reward: 2031.333 [686.000, 4596.000] - loss: 571.468 - mean_absolute_error: 17.583 - mean_q: 23.300 - mean_eps: 0.897 - ale.lives: 2.067\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.4516\n",
      "11 episodes - episode_reward: 2921.000 [196.000, 12758.000] - loss: 577.842 - mean_absolute_error: 19.967 - mean_q: 26.235 - mean_eps: 0.888 - ale.lives: 1.987\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.0218\n",
      "10 episodes - episode_reward: 2024.000 [416.000, 3271.000] - loss: 666.902 - mean_absolute_error: 22.582 - mean_q: 29.647 - mean_eps: 0.879 - ale.lives: 2.030\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.6274\n",
      "8 episodes - episode_reward: 3199.875 [973.000, 6723.000] - loss: 555.737 - mean_absolute_error: 23.463 - mean_q: 29.151 - mean_eps: 0.870 - ale.lives: 2.012\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.2127\n",
      "10 episodes - episode_reward: 1855.800 [739.000, 2906.000] - loss: 582.979 - mean_absolute_error: 28.828 - mean_q: 35.216 - mean_eps: 0.861 - ale.lives: 2.061\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.7782\n",
      "10 episodes - episode_reward: 3196.100 [1113.000, 4715.000] - loss: 503.505 - mean_absolute_error: 32.263 - mean_q: 38.301 - mean_eps: 0.852 - ale.lives: 2.039\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.7663\n",
      "11 episodes - episode_reward: 1591.545 [657.000, 2837.000] - loss: 563.031 - mean_absolute_error: 35.603 - mean_q: 42.667 - mean_eps: 0.843 - ale.lives: 1.993\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 2.0037\n",
      "10 episodes - episode_reward: 1728.800 [326.000, 3466.000] - loss: 549.323 - mean_absolute_error: 43.732 - mean_q: 51.052 - mean_eps: 0.834 - ale.lives: 2.115\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.1886\n",
      "9 episodes - episode_reward: 2572.111 [750.000, 4180.000] - loss: 411.582 - mean_absolute_error: 48.708 - mean_q: 55.954 - mean_eps: 0.825 - ale.lives: 2.105\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.7666\n",
      "10 episodes - episode_reward: 2832.900 [610.000, 5964.000] - loss: 590.127 - mean_absolute_error: 53.849 - mean_q: 62.291 - mean_eps: 0.816 - ale.lives: 1.880\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.9115\n",
      "9 episodes - episode_reward: 1960.889 [550.000, 3760.000] - loss: 575.145 - mean_absolute_error: 51.728 - mean_q: 59.805 - mean_eps: 0.807 - ale.lives: 2.226\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.4583\n",
      "11 episodes - episode_reward: 2290.455 [350.000, 3865.000] - loss: 479.147 - mean_absolute_error: 56.176 - mean_q: 63.504 - mean_eps: 0.798 - ale.lives: 2.057\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 2.5647\n",
      "11 episodes - episode_reward: 2501.364 [751.000, 5416.000] - loss: 532.976 - mean_absolute_error: 55.098 - mean_q: 63.069 - mean_eps: 0.789 - ale.lives: 2.099\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.8557\n",
      "10 episodes - episode_reward: 1623.400 [434.000, 3200.000] - loss: 505.377 - mean_absolute_error: 62.183 - mean_q: 71.274 - mean_eps: 0.780 - ale.lives: 2.009\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 3.0348\n",
      "9 episodes - episode_reward: 3478.667 [779.000, 7865.000] - loss: 617.056 - mean_absolute_error: 65.518 - mean_q: 74.828 - mean_eps: 0.771 - ale.lives: 1.911\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 2.5215\n",
      "9 episodes - episode_reward: 2883.444 [1095.000, 4851.000] - loss: 625.606 - mean_absolute_error: 67.222 - mean_q: 76.278 - mean_eps: 0.762 - ale.lives: 1.936\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.4379\n",
      "9 episodes - episode_reward: 2424.667 [303.000, 5486.000] - loss: 528.295 - mean_absolute_error: 67.678 - mean_q: 76.240 - mean_eps: 0.753 - ale.lives: 2.062\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 2.1665\n",
      "12 episodes - episode_reward: 1926.750 [510.000, 5170.000] - loss: 534.645 - mean_absolute_error: 70.372 - mean_q: 79.464 - mean_eps: 0.744 - ale.lives: 2.100\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 2.4899\n",
      "done, took 1745.881 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn2.fit(env2, nb_steps=300000, visualize=False, verbose=1) #callbacks=[save_at_episode()]\n",
    "dqn2.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model2_weights_v2',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "u9J6XulxKIpA",
    "outputId": "23ee8af4-3223-4508-bda5-caace9a04a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 2099.000, steps: 1500\n",
      "Episode 2: reward: 2727.000, steps: 1945\n",
      "Episode 3: reward: 1798.000, steps: 1234\n",
      "Episode 4: reward: 333.000, steps: 1131\n",
      "Episode 5: reward: 1121.000, steps: 748\n",
      "Episode 6: reward: 830.000, steps: 978\n",
      "Episode 7: reward: 1443.000, steps: 1245\n",
      "Episode 8: reward: 1998.000, steps: 1289\n",
      "Episode 9: reward: 1641.000, steps: 956\n",
      "Episode 10: reward: 349.000, steps: 1048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93e5b6f6a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn2.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model2_weights_v2')\n",
    "dqn2.test(env2, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V8unQng14c_H",
    "outputId": "08783b8f-bf41-49a7-d79f-8f80d288bb51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shared_weights, shared_bias = model1.get_layer('shared_layer').weights\n",
    "sns.distplot(K.eval(shared_weights).ravel())\n",
    "plt.savefig('/content/drive/My Drive/Colab Notebooks/Game Data/shared_weights_after_v2.jpg')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LN8xtMqI7g22"
   },
   "outputs": [],
   "source": [
    "dqn1.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_v2_changed',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "FfUsmBw77hmE",
    "outputId": "58baa97e-e38b-4b66-a06a-f31db21acedf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 50.000, steps: 261\n",
      "Episode 2: reward: 100.000, steps: 405\n",
      "Episode 3: reward: 75.000, steps: 362\n",
      "Episode 4: reward: 225.000, steps: 2242\n",
      "Episode 5: reward: 0.000, steps: 394\n",
      "Episode 6: reward: 25.000, steps: 382\n",
      "Episode 7: reward: 125.000, steps: 1017\n",
      "Episode 8: reward: 0.000, steps: 266\n",
      "Episode 9: reward: 50.000, steps: 398\n",
      "Episode 10: reward: 75.000, steps: 293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75e804a9b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn1.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_v2_changed')\n",
    "dqn1.test(env1, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4nuVUQWWOiw"
   },
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKtA_BmjIZSF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SharedArchitecture_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
