{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "Ygv2OXM-VKiS",
    "outputId": "2b4ce0dc-eae2-4743-aba1-c7fe3bfe7762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p-ZucKRlV31F",
    "outputId": "195d3919-5142-4ad1-ea1a-80e0e12ea634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-rl\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
      "\r",
      "\u001b[K     |████████                        | 10kB 18.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 40kB 2.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.17.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.3.2)\n",
      "Building wheels for collected packages: keras-rl\n",
      "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48379 sha256=674bf43e29b61b730e1cf879ba85d166676467f38d603666cd933de877831d15\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
      "Successfully built keras-rl\n",
      "Installing collected packages: keras-rl\n",
      "Successfully installed keras-rl-0.4.2\n",
      "Collecting keras==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
      "Installing collected packages: keras\n",
      "  Found existing installation: Keras 2.2.5\n",
      "    Uninstalling Keras-2.2.5:\n",
      "      Successfully uninstalled Keras-2.2.5\n",
      "Successfully installed keras-2.2.4\n",
      "Collecting tensorflow-gpu==1.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
      "\u001b[K     |████████████████████████████████| 345.2MB 49kB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K     |████████████████████████████████| 368kB 38.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 29.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.6)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.17.4)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
      "Collecting mock>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (41.6.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.16.0)\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow-gpu\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl\n",
    "!pip install keras==2.2.4\n",
    "!pip install tensorflow-gpu==1.13.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "MGq9Tc58V6Kz",
    "outputId": "25e8b78c-aade-4146-c1c1-8f0d5201e76b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kaboom', 'time_pilot', 'chopper_command', 'asterix', 'ms_pacman', 'adventure', 'elevator_action', 'kung_fu_master', 'robotank', 'boxing', 'jamesbond', 'centipede', 'journey_escape', 'riverraid', 'defender', 'pooyan', 'crazy_climber', 'zaxxon', 'pong', 'amidar', 'wizard_of_wor', 'fishing_derby', 'gravitar', 'kangaroo', 'road_runner', 'bank_heist', 'gopher', 'battle_zone', 'hero', 'star_gunner', 'qbert', 'breakout', 'space_invaders', 'phoenix', 'solaris', 'tennis', 'asteroids', 'frostbite', 'assault', 'double_dunk', 'alien', 'seaquest', 'carnival', 'demon_attack', 'atlantis', 'up_n_down', 'ice_hockey', 'air_raid', 'freeway', 'krull', 'private_eye', 'venture', 'beam_rider', 'pitfall', 'name_this_game', 'video_pinball', 'bowling', 'yars_revenge', 'berzerk', 'tutankham', 'montezuma_revenge', 'skiing', 'enduro']\n",
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "import atari_py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "\n",
    "print(atari_py.list_games())\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQ9Zw92JV8bd"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('float32')/255 \n",
    "\n",
    "    #def process_state_batch(self, batch):\n",
    "     #   processed_batch = batch.astype('float32') / 255.\n",
    "      #  return processed_batch\n",
    "\n",
    "    #def process_reward(self, reward):\n",
    "     #   return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD0PFtvGV-Wj"
   },
   "outputs": [],
   "source": [
    "#game = 'BeamRider-v0' #9\n",
    "#game = 'Breakout-v0' #4\n",
    "#game = 'Solaris-v0' #18\n",
    "#game = 'DemonAttack-v0' #6\n",
    "#game = 'Centipede-v0' #18\n",
    "\n",
    "game1 = 'AirRaid-v0' #6\n",
    "game2 = 'BankHeist-v0' #18\n",
    "\n",
    "\n",
    "env1 = gym.make(game1, full_action_space=True)\n",
    "np.random.seed(123)\n",
    "env1.seed(123)\n",
    "\n",
    "env2 = gym.make(game2, full_action_space=True)\n",
    "np.random.seed(123)\n",
    "env2.seed(123)\n",
    "\n",
    "\n",
    "if (env1.action_space.n == env2.action_space.n):\n",
    "  nb_actions = env1.action_space.n\n",
    "else:\n",
    "  raise RuntimeError('Actions does not match for 2 enviroments.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "id": "WGxWiFw1WBIE",
    "outputId": "d8cdac28-4061-40f8-e8fc-0b73e0777969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "shared_layer (Dense)         (None, 18)                9234      \n",
      "=================================================================\n",
      "Total params: 1,693,362\n",
      "Trainable params: 1,693,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "shared_layer (Dense)         (None, 18)                9234      \n",
      "=================================================================\n",
      "Total params: 1,693,362\n",
      "Trainable params: 1,693,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "\n",
    "# CNN for environment 1\n",
    "env1_input_layer = Input(shape=input_shape)\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    env1_permute = Permute((2, 3, 1))(env1_input_layer)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    env1_permute = Permute((1, 2, 3))(env1_input_layer)\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering for environment 1.')\n",
    "\n",
    "env1_Conv_1 = Convolution2D(32, (8, 8), strides=(4, 4), activation = 'relu')(env1_permute)\n",
    "env1_Conv_2 = Convolution2D(64, (4, 4), strides=(2, 2), activation = 'relu')(env1_Conv_1)\n",
    "env1_Conv_3 = Convolution2D(64, (3, 3), strides=(1, 1), activation = 'relu')(env1_Conv_2)\n",
    "env1_flatten = Flatten()(env1_Conv_3)\n",
    "env1_dense = Dense(512, activation = 'relu')(env1_flatten)\n",
    "\n",
    "\n",
    "# CNN for environment 2\n",
    "env2_input_layer = Input(shape=input_shape)\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    env2_permute = Permute((2, 3, 1))(env2_input_layer)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    env2_permute = Permute((1, 2, 3))(env2_input_layer)\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering for environment 2.')\n",
    "    \n",
    "env2_Conv_1 = Convolution2D(32, (8, 8), strides=(4, 4), activation = 'relu')(env2_permute)\n",
    "env2_Conv_2 = Convolution2D(64, (4, 4), strides=(2, 2), activation = 'relu')(env2_Conv_1)\n",
    "env2_Conv_3 = Convolution2D(64, (3, 3), strides=(1, 1), activation = 'relu')(env2_Conv_2)\n",
    "env2_flatten = Flatten()(env2_Conv_3)\n",
    "env2_dense = Dense(512, activation = 'relu')(env2_flatten)\n",
    "\n",
    "shared_layer = Dense(nb_actions, activation = 'linear', name = 'shared_layer')\n",
    "\n",
    "env1_output = shared_layer(env1_dense)\n",
    "env2_output = shared_layer(env2_dense)\n",
    "\n",
    "\n",
    "model1 = Model(inputs = [env1_input_layer], outputs = [env1_output]) \n",
    "model2 =  Model(inputs = [env2_input_layer], outputs = [env2_output])\n",
    "\n",
    "print(model1.summary())\n",
    "plot_model(model1)\n",
    "print(model2.summary())\n",
    "plot_model(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zxrYJtNWDXl"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn1 = DQNAgent(model=model1, nb_actions=nb_actions, policy=policy, memory=memory, processor=processor, nb_steps_warmup=50000, target_model_update=10000,train_interval=4)\n",
    "dqn1.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c2fU46AYWKDZ",
    "outputId": "b6d457b8-a1cb-461e-e325-145ab457fa0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.8500\n",
      "13 episodes - episode_reward: 650.000 [150.000, 1900.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 25s 3ms/step - reward: 0.8475\n",
      "10 episodes - episode_reward: 597.500 [200.000, 1275.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 25s 3ms/step - reward: 0.6050\n",
      "12 episodes - episode_reward: 712.500 [25.000, 3325.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 25s 3ms/step - reward: 0.7075\n",
      "13 episodes - episode_reward: 440.385 [200.000, 725.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 25s 2ms/step - reward: 0.6725\n",
      "12 episodes - episode_reward: 643.750 [225.000, 1800.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "    1/10000 [..............................] - ETA: 1:31 - reward: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.7675\n",
      "15 episodes - episode_reward: 526.667 [0.000, 1125.000] - loss: 26.447 - mean_absolute_error: 0.607 - mean_q: 1.209 - mean_eps: 0.951 - ale.lives: 0.999\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.7525\n",
      "16 episodes - episode_reward: 478.125 [175.000, 1000.000] - loss: 29.129 - mean_absolute_error: 1.446 - mean_q: 1.953 - mean_eps: 0.942 - ale.lives: 0.998\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.7925\n",
      "10 episodes - episode_reward: 630.000 [175.000, 1025.000] - loss: 25.356 - mean_absolute_error: 2.250 - mean_q: 2.962 - mean_eps: 0.933 - ale.lives: 0.999\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.8050\n",
      "16 episodes - episode_reward: 575.000 [225.000, 1800.000] - loss: 28.156 - mean_absolute_error: 3.163 - mean_q: 4.147 - mean_eps: 0.924 - ale.lives: 0.998\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 0.7100\n",
      "11 episodes - episode_reward: 663.636 [50.000, 1500.000] - loss: 28.248 - mean_absolute_error: 4.401 - mean_q: 5.664 - mean_eps: 0.915 - ale.lives: 0.999\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.7275\n",
      "14 episodes - episode_reward: 510.714 [100.000, 1125.000] - loss: 27.264 - mean_absolute_error: 5.738 - mean_q: 7.270 - mean_eps: 0.906 - ale.lives: 0.999\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.8825\n",
      "12 episodes - episode_reward: 741.667 [0.000, 1600.000] - loss: 30.766 - mean_absolute_error: 6.703 - mean_q: 8.256 - mean_eps: 0.897 - ale.lives: 0.999\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.7375\n",
      "14 episodes - episode_reward: 514.286 [125.000, 2175.000] - loss: 28.260 - mean_absolute_error: 7.669 - mean_q: 9.325 - mean_eps: 0.888 - ale.lives: 0.999\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: 0.6325\n",
      "14 episodes - episode_reward: 475.000 [0.000, 1450.000] - loss: 27.796 - mean_absolute_error: 8.759 - mean_q: 10.498 - mean_eps: 0.879 - ale.lives: 0.999\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 0.7625\n",
      "14 episodes - episode_reward: 539.286 [225.000, 1100.000] - loss: 30.087 - mean_absolute_error: 10.206 - mean_q: 12.168 - mean_eps: 0.870 - ale.lives: 0.999\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 0.7950\n",
      "16 episodes - episode_reward: 503.125 [200.000, 1025.000] - loss: 30.005 - mean_absolute_error: 12.179 - mean_q: 14.401 - mean_eps: 0.861 - ale.lives: 0.998\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 0.7800\n",
      "13 episodes - episode_reward: 459.615 [175.000, 975.000] - loss: 29.066 - mean_absolute_error: 13.781 - mean_q: 16.166 - mean_eps: 0.852 - ale.lives: 0.999\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 0.7900\n",
      "12 episodes - episode_reward: 795.833 [250.000, 2675.000] - loss: 29.424 - mean_absolute_error: 14.838 - mean_q: 17.300 - mean_eps: 0.843 - ale.lives: 0.999\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.7775\n",
      "18 episodes - episode_reward: 450.000 [50.000, 850.000] - loss: 29.057 - mean_absolute_error: 16.007 - mean_q: 18.547 - mean_eps: 0.834 - ale.lives: 0.998\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.7425\n",
      "16 episodes - episode_reward: 451.562 [50.000, 1375.000] - loss: 28.880 - mean_absolute_error: 17.471 - mean_q: 20.089 - mean_eps: 0.825 - ale.lives: 0.998\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.9175\n",
      "12 episodes - episode_reward: 752.083 [375.000, 1725.000] - loss: 31.056 - mean_absolute_error: 18.474 - mean_q: 21.135 - mean_eps: 0.816 - ale.lives: 0.999\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.8825\n",
      "13 episodes - episode_reward: 696.154 [200.000, 1350.000] - loss: 28.608 - mean_absolute_error: 19.377 - mean_q: 22.038 - mean_eps: 0.807 - ale.lives: 0.999\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.9525\n",
      "11 episodes - episode_reward: 711.364 [375.000, 1375.000] - loss: 31.367 - mean_absolute_error: 20.643 - mean_q: 23.572 - mean_eps: 0.798 - ale.lives: 0.999\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 0.7850\n",
      "16 episodes - episode_reward: 567.188 [100.000, 2375.000] - loss: 30.586 - mean_absolute_error: 22.452 - mean_q: 25.407 - mean_eps: 0.789 - ale.lives: 0.998\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: 0.8425\n",
      "12 episodes - episode_reward: 731.250 [200.000, 1675.000] - loss: 34.239 - mean_absolute_error: 23.247 - mean_q: 26.288 - mean_eps: 0.780 - ale.lives: 0.999\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 0.9800\n",
      "10 episodes - episode_reward: 987.500 [375.000, 3100.000] - loss: 34.087 - mean_absolute_error: 24.516 - mean_q: 27.631 - mean_eps: 0.771 - ale.lives: 0.999\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: 0.8675\n",
      "15 episodes - episode_reward: 586.667 [175.000, 1300.000] - loss: 32.618 - mean_absolute_error: 26.294 - mean_q: 29.649 - mean_eps: 0.762 - ale.lives: 0.999\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: 0.8300\n",
      "12 episodes - episode_reward: 643.750 [150.000, 1625.000] - loss: 33.648 - mean_absolute_error: 27.602 - mean_q: 31.108 - mean_eps: 0.753 - ale.lives: 0.999\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: 0.9175\n",
      "11 episodes - episode_reward: 754.545 [275.000, 1575.000] - loss: 34.205 - mean_absolute_error: 28.751 - mean_q: 32.282 - mean_eps: 0.744 - ale.lives: 0.999\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 0.8325\n",
      "done, took 1842.661 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn1.fit(env1, nb_steps=300000, visualize=False, verbose=1) \n",
    "dqn1.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_dissimilar',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "ETX3ntuYWL2A",
    "outputId": "04285d35-2cb3-4239-d4fd-2f3b9930a02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 150.000, steps: 305\n",
      "Episode 2: reward: 0.000, steps: 287\n",
      "Episode 3: reward: 150.000, steps: 1028\n",
      "Episode 4: reward: 450.000, steps: 514\n",
      "Episode 5: reward: 0.000, steps: 266\n",
      "Episode 6: reward: 200.000, steps: 332\n",
      "Episode 7: reward: 200.000, steps: 602\n",
      "Episode 8: reward: 275.000, steps: 426\n",
      "Episode 9: reward: 75.000, steps: 233\n",
      "Episode 10: reward: 425.000, steps: 815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff7ccae048>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dqn1.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_dissimilar')\n",
    "dqn1.test(env1, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wcCV4bWa06jk",
    "outputId": "7941f408-5941-49e9-9e52-55d2206e5ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "shared_weights, shared_bias = model1.get_layer('shared_layer').weights\n",
    "sns.distplot(K.eval(shared_weights).ravel())\n",
    "plt.savefig('/content/drive/My Drive/Colab Notebooks/Game Data/shared_weights_before_dissimilar.jpg')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uxnc2Z7v4AqS"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn2 = DQNAgent(model=model2, nb_actions=nb_actions, policy=policy, memory=memory, processor=processor, nb_steps_warmup=50000, target_model_update=10000,train_interval=4)\n",
    "dqn2.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LdllTK414OS4",
    "outputId": "bb54e6a1-0183-4e4b-baa6-a5b82fb6e279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 30s 3ms/step - reward: 0.0270\n",
      "14 episodes - episode_reward: 17.857 [0.000, 30.000] - ale.lives: 1.995\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.0260\n",
      "15 episodes - episode_reward: 17.333 [0.000, 30.000] - ale.lives: 2.084\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.0260\n",
      "14 episodes - episode_reward: 19.286 [0.000, 40.000] - ale.lives: 1.936\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.0320\n",
      "15 episodes - episode_reward: 22.000 [0.000, 70.000] - ale.lives: 2.022\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.0290\n",
      "14 episodes - episode_reward: 19.286 [0.000, 40.000] - ale.lives: 1.982\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 0.0200\n",
      "15 episodes - episode_reward: 13.333 [0.000, 30.000] - loss: 0.148 - mean_absolute_error: 0.096 - mean_q: 0.141 - mean_eps: 0.951 - ale.lives: 2.100\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0240\n",
      "15 episodes - episode_reward: 16.667 [0.000, 30.000] - loss: 0.158 - mean_absolute_error: 0.144 - mean_q: 0.199 - mean_eps: 0.942 - ale.lives: 2.191\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0210\n",
      "14 episodes - episode_reward: 15.000 [10.000, 30.000] - loss: 0.129 - mean_absolute_error: 0.180 - mean_q: 0.263 - mean_eps: 0.933 - ale.lives: 2.018\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0250\n",
      "14 episodes - episode_reward: 18.571 [10.000, 40.000] - loss: 0.100 - mean_absolute_error: 0.232 - mean_q: 0.354 - mean_eps: 0.924 - ale.lives: 1.928\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0220\n",
      "15 episodes - episode_reward: 14.667 [0.000, 30.000] - loss: 0.136 - mean_absolute_error: 0.266 - mean_q: 0.403 - mean_eps: 0.915 - ale.lives: 2.034\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0230\n",
      "15 episodes - episode_reward: 15.333 [10.000, 30.000] - loss: 0.174 - mean_absolute_error: 0.334 - mean_q: 0.487 - mean_eps: 0.906 - ale.lives: 1.946\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0200\n",
      "15 episodes - episode_reward: 13.333 [0.000, 30.000] - loss: 0.176 - mean_absolute_error: 0.421 - mean_q: 0.585 - mean_eps: 0.897 - ale.lives: 2.070\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0250\n",
      "13 episodes - episode_reward: 18.462 [0.000, 80.000] - loss: 0.163 - mean_absolute_error: 0.474 - mean_q: 0.642 - mean_eps: 0.888 - ale.lives: 2.102\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0230\n",
      "17 episodes - episode_reward: 13.529 [0.000, 40.000] - loss: 0.180 - mean_absolute_error: 0.550 - mean_q: 0.718 - mean_eps: 0.879 - ale.lives: 1.974\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 0.0160\n",
      "15 episodes - episode_reward: 10.000 [0.000, 30.000] - loss: 0.160 - mean_absolute_error: 0.563 - mean_q: 0.726 - mean_eps: 0.870 - ale.lives: 1.971\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 0.0230\n",
      "16 episodes - episode_reward: 14.375 [0.000, 40.000] - loss: 0.171 - mean_absolute_error: 0.629 - mean_q: 0.788 - mean_eps: 0.861 - ale.lives: 2.035\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 0.0240\n",
      "13 episodes - episode_reward: 18.462 [10.000, 40.000] - loss: 0.177 - mean_absolute_error: 0.702 - mean_q: 0.869 - mean_eps: 0.852 - ale.lives: 2.029\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: 0.0260\n",
      "16 episodes - episode_reward: 17.500 [0.000, 40.000] - loss: 0.156 - mean_absolute_error: 0.732 - mean_q: 0.901 - mean_eps: 0.843 - ale.lives: 2.016\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 0.0280\n",
      "14 episodes - episode_reward: 19.286 [0.000, 40.000] - loss: 0.181 - mean_absolute_error: 0.808 - mean_q: 0.987 - mean_eps: 0.834 - ale.lives: 1.960\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 0.0310\n",
      "15 episodes - episode_reward: 20.667 [0.000, 60.000] - loss: 0.152 - mean_absolute_error: 0.902 - mean_q: 1.082 - mean_eps: 0.825 - ale.lives: 2.049\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.0280\n",
      "15 episodes - episode_reward: 18.667 [10.000, 40.000] - loss: 0.153 - mean_absolute_error: 0.974 - mean_q: 1.173 - mean_eps: 0.816 - ale.lives: 2.013\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.0350\n",
      "15 episodes - episode_reward: 22.667 [0.000, 50.000] - loss: 0.191 - mean_absolute_error: 1.075 - mean_q: 1.293 - mean_eps: 0.807 - ale.lives: 1.954\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.0300\n",
      "14 episodes - episode_reward: 22.143 [10.000, 40.000] - loss: 0.181 - mean_absolute_error: 1.148 - mean_q: 1.365 - mean_eps: 0.798 - ale.lives: 2.082\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.0460\n",
      "15 episodes - episode_reward: 30.000 [10.000, 50.000] - loss: 0.175 - mean_absolute_error: 1.288 - mean_q: 1.514 - mean_eps: 0.789 - ale.lives: 2.155\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.0440\n",
      "12 episodes - episode_reward: 35.833 [10.000, 70.000] - loss: 0.177 - mean_absolute_error: 1.367 - mean_q: 1.607 - mean_eps: 0.780 - ale.lives: 2.014\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 0.0480\n",
      "16 episodes - episode_reward: 30.625 [20.000, 50.000] - loss: 0.201 - mean_absolute_error: 1.422 - mean_q: 1.678 - mean_eps: 0.771 - ale.lives: 1.994\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 0.0520\n",
      "14 episodes - episode_reward: 37.143 [20.000, 80.000] - loss: 0.225 - mean_absolute_error: 1.569 - mean_q: 1.842 - mean_eps: 0.762 - ale.lives: 2.056\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.0470\n",
      "14 episodes - episode_reward: 34.286 [0.000, 110.000] - loss: 0.235 - mean_absolute_error: 1.735 - mean_q: 2.029 - mean_eps: 0.753 - ale.lives: 2.147\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0490\n",
      "15 episodes - episode_reward: 30.000 [10.000, 50.000] - loss: 0.219 - mean_absolute_error: 1.766 - mean_q: 2.053 - mean_eps: 0.744 - ale.lives: 2.094\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 0.0590\n",
      "done, took 1995.534 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn2.fit(env2, nb_steps=300000, visualize=False, verbose=1)\n",
    "dqn2.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model2_weights_dissimilar',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "u9J6XulxKIpA",
    "outputId": "94a23555-565a-455f-9fb0-41d09e07990d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 170.000, steps: 938\n",
      "Episode 2: reward: 80.000, steps: 626\n",
      "Episode 3: reward: 140.000, steps: 737\n",
      "Episode 4: reward: 140.000, steps: 791\n",
      "Episode 5: reward: 120.000, steps: 636\n",
      "Episode 6: reward: 160.000, steps: 786\n",
      "Episode 7: reward: 190.000, steps: 788\n",
      "Episode 8: reward: 60.000, steps: 538\n",
      "Episode 9: reward: 170.000, steps: 892\n",
      "Episode 10: reward: 50.000, steps: 468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe11973390>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn2.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model2_weights_dissimilar')\n",
    "dqn2.test(env2, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V8unQng14c_H",
    "outputId": "3ee54282-113d-4dda-9417-5bcd8c8439bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shared_weights, shared_bias = model1.get_layer('shared_layer').weights\n",
    "sns.distplot(K.eval(shared_weights).ravel())\n",
    "plt.savefig('/content/drive/My Drive/Colab Notebooks/Game Data/shared_weights_after_dissimilar.jpg')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKc5-MeI6Map"
   },
   "outputs": [],
   "source": [
    "dqn1.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_dissimilar_changed',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "Tr4tIAnt6WCD",
    "outputId": "1fcca69c-23d4-4512-eea4-e138c6524c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 650.000, steps: 1149\n",
      "Episode 2: reward: 825.000, steps: 984\n",
      "Episode 3: reward: 900.000, steps: 1483\n",
      "Episode 4: reward: 1325.000, steps: 2152\n",
      "Episode 5: reward: 1000.000, steps: 1574\n",
      "Episode 6: reward: 575.000, steps: 847\n",
      "Episode 7: reward: 2000.000, steps: 2728\n",
      "Episode 8: reward: 75.000, steps: 191\n",
      "Episode 9: reward: 975.000, steps: 1452\n",
      "Episode 10: reward: 775.000, steps: 1392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcd4bad860>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn1.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_dissimilar_changed')\n",
    "dqn1.test(env1, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4nuVUQWWOiw"
   },
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKtA_BmjIZSF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Dissimilar_Game_SharedArchitecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
