{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "Ygv2OXM-VKiS",
    "outputId": "e3dacca4-bb2d-4745-d7f6-487349301c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p-ZucKRlV31F",
    "outputId": "6ce23e3b-c01b-4964-a125-db1bfc9ac0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-rl\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
      "\r",
      "\u001b[K     |████████                        | 10kB 21.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 40kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.17.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.3.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
      "Building wheels for collected packages: keras-rl\n",
      "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48379 sha256=9c5abc625fc3d81bd59a5f940952c5485385703aac17fc7d91fb78bf19eae28e\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
      "Successfully built keras-rl\n",
      "Installing collected packages: keras-rl\n",
      "Successfully installed keras-rl-0.4.2\n",
      "Collecting keras==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
      "Installing collected packages: keras\n",
      "  Found existing installation: Keras 2.2.5\n",
      "    Uninstalling Keras-2.2.5:\n",
      "      Successfully uninstalled Keras-2.2.5\n",
      "Successfully installed keras-2.2.4\n",
      "Collecting tensorflow-gpu==1.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
      "\u001b[K     |████████████████████████████████| 345.2MB 35kB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.17.4)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K     |████████████████████████████████| 368kB 51.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.6)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 30.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (41.6.0)\n",
      "Collecting mock>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.16.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow-gpu\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl\n",
    "!pip install keras==2.2.4\n",
    "!pip install tensorflow-gpu==1.13.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "MGq9Tc58V6Kz",
    "outputId": "affc1295-d6db-43ce-c748-3f5106decd06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "import atari_py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "\n",
    "#print(atari_py.list_games())\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQ9Zw92JV8bd"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('float32')/255 \n",
    "\n",
    "    #def process_reward(self, reward):\n",
    "     #   return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD0PFtvGV-Wj"
   },
   "outputs": [],
   "source": [
    "#game = 'BeamRider-v0' #9\n",
    "#game = 'Breakout-v0' #4\n",
    "#game = 'Solaris-v0' #18\n",
    "#game = 'DemonAttack-v0' #6\n",
    "#game = 'AirRaid-v0' #6\n",
    "\n",
    "game1 = 'AirRaid-v0' #6\n",
    "game2 = 'Centipede-v0' #18\n",
    "\n",
    "env1 = gym.make(game1, full_action_space=True)\n",
    "np.random.seed(123)\n",
    "env1.seed(123)\n",
    "\n",
    "env2 = gym.make(game2, full_action_space=True)\n",
    "np.random.seed(123)\n",
    "env2.seed(123)\n",
    "\n",
    "\n",
    "if (env1.action_space.n == env2.action_space.n):\n",
    "  nb_actions = env1.action_space.n\n",
    "else:\n",
    "  raise RuntimeError('Actions does not match for 2 enviroments.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "id": "WGxWiFw1WBIE",
    "outputId": "d4715bb9-c2a2-4633-f6ca-5166ff2af92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "shared_layer (Dense)         (None, 18)                9234      \n",
      "=================================================================\n",
      "Total params: 1,693,362\n",
      "Trainable params: 1,693,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "shared_layer (Dense)         (None, 18)                9234      \n",
      "=================================================================\n",
      "Total params: 1,693,362\n",
      "Trainable params: 1,693,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "\n",
    "# CNN for environment 1\n",
    "env1_input_layer = Input(shape=input_shape)\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    env1_permute = Permute((2, 3, 1))(env1_input_layer)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    env1_permute = Permute((1, 2, 3))(env1_input_layer)\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering for environment 1.')\n",
    "\n",
    "env1_Conv_1 = Convolution2D(32, (8, 8), strides=(4, 4), activation = 'relu')(env1_permute)\n",
    "env1_Conv_2 = Convolution2D(64, (4, 4), strides=(2, 2), activation = 'relu')(env1_Conv_1)\n",
    "env1_Conv_3 = Convolution2D(64, (3, 3), strides=(1, 1), activation = 'relu')(env1_Conv_2)\n",
    "env1_flatten = Flatten()(env1_Conv_3)\n",
    "env1_dense = Dense(512, activation = 'relu')(env1_flatten)\n",
    "\n",
    "\n",
    "# CNN for environment 2\n",
    "env2_input_layer = Input(shape=input_shape)\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    env2_permute = Permute((2, 3, 1))(env2_input_layer)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    env2_permute = Permute((1, 2, 3))(env2_input_layer)\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering for environment 2.')\n",
    "    \n",
    "env2_Conv_1 = Convolution2D(32, (8, 8), strides=(4, 4), activation = 'relu')(env2_permute)\n",
    "env2_Conv_2 = Convolution2D(64, (4, 4), strides=(2, 2), activation = 'relu')(env2_Conv_1)\n",
    "env2_Conv_3 = Convolution2D(64, (3, 3), strides=(1, 1), activation = 'relu')(env2_Conv_2)\n",
    "env2_flatten = Flatten()(env2_Conv_3)\n",
    "env2_dense = Dense(512, activation = 'relu')(env2_flatten)\n",
    "\n",
    "shared_layer = Dense(nb_actions, activation = 'linear', name = 'shared_layer')\n",
    "\n",
    "env1_output = shared_layer(env1_dense)\n",
    "env2_output = shared_layer(env2_dense)\n",
    "\n",
    "\n",
    "model1 = Model(inputs = [env1_input_layer], outputs = [env1_output]) \n",
    "model2 =  Model(inputs = [env2_input_layer], outputs = [env2_output])\n",
    "\n",
    "print(model1.summary())\n",
    "plot_model(model1)\n",
    "print(model2.summary())\n",
    "plot_model(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zxrYJtNWDXl"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn1 = DQNAgent(model=model1, nb_actions=nb_actions, policy=policy, memory=memory, processor=processor, nb_steps_warmup=50000, target_model_update=10000,train_interval=4)\n",
    "dqn1.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vwl-a6KZWHgQ"
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback as KerasCallback\n",
    "# class save_at_episode(KerasCallback):\n",
    "#     def on_episode_end(self, episode, logs={}):\n",
    "#         self.model.save_weights('drive/My Drive/Colab Notebooks/Game Data/Breakout_weights_v1_{}'.format(episode), overwrite=True)\n",
    "#         #print(\" saved episode:{}\".format(episode))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c2fU46AYWKDZ",
    "outputId": "185f1187-c37d-4760-f691-d305a9b7ab8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.8500\n",
      "13 episodes - episode_reward: 650.000 [150.000, 1900.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 0.8475\n",
      "10 episodes - episode_reward: 597.500 [200.000, 1275.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 26s 3ms/step - reward: 0.6050\n",
      "12 episodes - episode_reward: 712.500 [25.000, 3325.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 25s 3ms/step - reward: 0.7075\n",
      "13 episodes - episode_reward: 440.385 [200.000, 725.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 25s 3ms/step - reward: 0.6725\n",
      "12 episodes - episode_reward: 643.750 [225.000, 1800.000] - ale.lives: 0.999\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "    1/10000 [..............................] - ETA: 1:52 - reward: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.6300\n",
      "14 episodes - episode_reward: 469.643 [50.000, 1325.000] - loss: 28.384 - mean_absolute_error: 0.643 - mean_q: 1.108 - mean_eps: 0.951 - ale.lives: 0.999\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.7825\n",
      "11 episodes - episode_reward: 722.727 [50.000, 1750.000] - loss: 25.943 - mean_absolute_error: 1.375 - mean_q: 1.912 - mean_eps: 0.942 - ale.lives: 0.999\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.7500\n",
      "13 episodes - episode_reward: 551.923 [200.000, 1050.000] - loss: 26.918 - mean_absolute_error: 1.818 - mean_q: 2.514 - mean_eps: 0.933 - ale.lives: 0.999\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7875\n",
      "13 episodes - episode_reward: 623.077 [100.000, 1825.000] - loss: 25.748 - mean_absolute_error: 2.895 - mean_q: 3.990 - mean_eps: 0.924 - ale.lives: 0.999\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.7225\n",
      "13 episodes - episode_reward: 553.846 [300.000, 1575.000] - loss: 24.883 - mean_absolute_error: 4.508 - mean_q: 5.846 - mean_eps: 0.915 - ale.lives: 0.999\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.6925\n",
      "13 episodes - episode_reward: 496.154 [50.000, 1700.000] - loss: 26.652 - mean_absolute_error: 5.402 - mean_q: 6.724 - mean_eps: 0.906 - ale.lives: 0.999\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.7625\n",
      "15 episodes - episode_reward: 528.333 [75.000, 1250.000] - loss: 29.518 - mean_absolute_error: 5.750 - mean_q: 7.323 - mean_eps: 0.897 - ale.lives: 0.999\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8275\n",
      "15 episodes - episode_reward: 553.333 [50.000, 1525.000] - loss: 29.059 - mean_absolute_error: 6.807 - mean_q: 8.480 - mean_eps: 0.888 - ale.lives: 0.999\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.7850\n",
      "14 episodes - episode_reward: 483.929 [225.000, 1100.000] - loss: 29.666 - mean_absolute_error: 7.697 - mean_q: 9.426 - mean_eps: 0.879 - ale.lives: 0.999\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8000\n",
      "15 episodes - episode_reward: 608.333 [50.000, 2250.000] - loss: 28.484 - mean_absolute_error: 8.631 - mean_q: 10.391 - mean_eps: 0.870 - ale.lives: 0.999\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8275\n",
      "17 episodes - episode_reward: 486.765 [25.000, 1125.000] - loss: 31.751 - mean_absolute_error: 10.582 - mean_q: 12.778 - mean_eps: 0.861 - ale.lives: 0.998\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.9425\n",
      "15 episodes - episode_reward: 643.333 [150.000, 1625.000] - loss: 31.334 - mean_absolute_error: 12.271 - mean_q: 14.603 - mean_eps: 0.852 - ale.lives: 0.999\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8525\n",
      "17 episodes - episode_reward: 497.059 [75.000, 1600.000] - loss: 31.124 - mean_absolute_error: 13.011 - mean_q: 15.407 - mean_eps: 0.843 - ale.lives: 0.998\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.8900\n",
      "12 episodes - episode_reward: 741.667 [150.000, 1400.000] - loss: 31.695 - mean_absolute_error: 14.257 - mean_q: 16.770 - mean_eps: 0.834 - ale.lives: 0.999\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8325\n",
      "15 episodes - episode_reward: 560.000 [275.000, 950.000] - loss: 30.357 - mean_absolute_error: 15.492 - mean_q: 18.030 - mean_eps: 0.825 - ale.lives: 0.999\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8300\n",
      "15 episodes - episode_reward: 528.333 [50.000, 1275.000] - loss: 29.901 - mean_absolute_error: 16.144 - mean_q: 18.782 - mean_eps: 0.816 - ale.lives: 0.999\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.7975\n",
      "14 episodes - episode_reward: 582.143 [150.000, 1500.000] - loss: 33.375 - mean_absolute_error: 17.598 - mean_q: 20.456 - mean_eps: 0.807 - ale.lives: 0.999\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.8600\n",
      "10 episodes - episode_reward: 847.500 [275.000, 1650.000] - loss: 34.934 - mean_absolute_error: 19.433 - mean_q: 22.250 - mean_eps: 0.798 - ale.lives: 0.999\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.9425\n",
      "13 episodes - episode_reward: 719.231 [225.000, 1700.000] - loss: 36.781 - mean_absolute_error: 21.291 - mean_q: 24.343 - mean_eps: 0.789 - ale.lives: 0.999\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.9875\n",
      "10 episodes - episode_reward: 995.000 [50.000, 4250.000] - loss: 34.765 - mean_absolute_error: 22.900 - mean_q: 26.116 - mean_eps: 0.780 - ale.lives: 0.999\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.9725\n",
      "17 episodes - episode_reward: 539.706 [75.000, 950.000] - loss: 36.017 - mean_absolute_error: 24.534 - mean_q: 27.882 - mean_eps: 0.771 - ale.lives: 0.998\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.8625\n",
      "12 episodes - episode_reward: 775.000 [100.000, 1825.000] - loss: 33.858 - mean_absolute_error: 26.042 - mean_q: 29.517 - mean_eps: 0.762 - ale.lives: 0.999\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8075\n",
      "15 episodes - episode_reward: 478.333 [200.000, 825.000] - loss: 34.006 - mean_absolute_error: 27.894 - mean_q: 31.552 - mean_eps: 0.753 - ale.lives: 0.999\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.8625\n",
      "16 episodes - episode_reward: 585.938 [175.000, 1725.000] - loss: 35.695 - mean_absolute_error: 29.187 - mean_q: 33.053 - mean_eps: 0.744 - ale.lives: 0.998\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.8450\n",
      "done, took 1700.063 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn1.fit(env1, nb_steps=300000, visualize=False, verbose=1) #callbacks=[save_at_episode()]\n",
    "dqn1.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights',overwrite=True)\n",
    "#'/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "ETX3ntuYWL2A",
    "outputId": "267e7869-e175-466a-accc-40fd5e22824a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 25.000, steps: 204\n",
      "Episode 2: reward: 200.000, steps: 489\n",
      "Episode 3: reward: 400.000, steps: 466\n",
      "Episode 4: reward: 100.000, steps: 322\n",
      "Episode 5: reward: 100.000, steps: 384\n",
      "Episode 6: reward: 125.000, steps: 301\n",
      "Episode 7: reward: 225.000, steps: 378\n",
      "Episode 8: reward: 350.000, steps: 259\n",
      "Episode 9: reward: 75.000, steps: 317\n",
      "Episode 10: reward: 75.000, steps: 283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0404af4e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dqn1.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights')\n",
    "dqn1.test(env1, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wcCV4bWa06jk",
    "outputId": "3e84134b-0a54-43f7-e436-f5639beccbfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "shared_weights, shared_bias = model1.get_layer('shared_layer').weights\n",
    "sns.distplot(K.eval(shared_weights).ravel())\n",
    "plt.savefig('/content/drive/My Drive/Colab Notebooks/Game Data/shared_weights_before.jpg')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uxnc2Z7v4AqS"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn2 = DQNAgent(model=model2, nb_actions=nb_actions, policy=policy, memory=memory, processor=processor, nb_steps_warmup=50000, target_model_update=10000,train_interval=4)\n",
    "dqn2.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LdllTK414OS4",
    "outputId": "d18f53f4-fc49-44a9-8162-a5866f04ebe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 2.7536\n",
      "10 episodes - episode_reward: 2672.900 [1095.000, 6563.000] - ale.lives: 2.058\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 2.3132\n",
      "11 episodes - episode_reward: 2145.000 [857.000, 3913.000] - ale.lives: 2.027\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 2.8787\n",
      "10 episodes - episode_reward: 2901.000 [1085.000, 4963.000] - ale.lives: 2.038\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 2.2839\n",
      "11 episodes - episode_reward: 2027.727 [423.000, 2917.000] - ale.lives: 2.061\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 1.9387\n",
      "10 episodes - episode_reward: 1713.500 [246.000, 4775.000] - ale.lives: 2.134\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 2.6806\n",
      "11 episodes - episode_reward: 2463.455 [333.000, 6180.000] - loss: 525.740 - mean_absolute_error: 2.398 - mean_q: 4.535 - mean_eps: 0.951 - ale.lives: 1.994\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 2.3306\n",
      "12 episodes - episode_reward: 2158.333 [914.000, 4038.000] - loss: 497.596 - mean_absolute_error: 4.903 - mean_q: 7.270 - mean_eps: 0.942 - ale.lives: 2.095\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.2557\n",
      "11 episodes - episode_reward: 2027.091 [964.000, 3150.000] - loss: 535.158 - mean_absolute_error: 10.565 - mean_q: 14.388 - mean_eps: 0.933 - ale.lives: 2.128\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.6988\n",
      "10 episodes - episode_reward: 2647.000 [739.000, 7604.000] - loss: 521.511 - mean_absolute_error: 10.477 - mean_q: 13.623 - mean_eps: 0.924 - ale.lives: 2.145\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.9154\n",
      "11 episodes - episode_reward: 1804.636 [432.000, 4609.000] - loss: 519.064 - mean_absolute_error: 13.990 - mean_q: 18.346 - mean_eps: 0.915 - ale.lives: 1.987\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 1.9501\n",
      "10 episodes - episode_reward: 1865.700 [1003.000, 3132.000] - loss: 364.324 - mean_absolute_error: 19.583 - mean_q: 24.617 - mean_eps: 0.906 - ale.lives: 1.959\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.3210\n",
      "10 episodes - episode_reward: 2408.100 [396.000, 5373.000] - loss: 534.772 - mean_absolute_error: 22.961 - mean_q: 28.656 - mean_eps: 0.897 - ale.lives: 2.163\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.0879\n",
      "10 episodes - episode_reward: 1893.200 [723.000, 3308.000] - loss: 435.354 - mean_absolute_error: 32.379 - mean_q: 39.442 - mean_eps: 0.888 - ale.lives: 2.040\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.7131\n",
      "9 episodes - episode_reward: 3203.333 [2239.000, 5829.000] - loss: 450.938 - mean_absolute_error: 36.284 - mean_q: 43.363 - mean_eps: 0.879 - ale.lives: 2.056\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.1577\n",
      "10 episodes - episode_reward: 2148.900 [775.000, 3972.000] - loss: 461.850 - mean_absolute_error: 40.889 - mean_q: 48.185 - mean_eps: 0.870 - ale.lives: 2.002\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 2.9555\n",
      "9 episodes - episode_reward: 2494.667 [1164.000, 5947.000] - loss: 466.292 - mean_absolute_error: 51.446 - mean_q: 60.019 - mean_eps: 0.861 - ale.lives: 1.942\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.3150\n",
      "10 episodes - episode_reward: 2889.900 [962.000, 7591.000] - loss: 597.284 - mean_absolute_error: 52.425 - mean_q: 61.969 - mean_eps: 0.852 - ale.lives: 2.015\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.4493\n",
      "10 episodes - episode_reward: 2441.300 [1477.000, 3287.000] - loss: 526.536 - mean_absolute_error: 59.853 - mean_q: 68.919 - mean_eps: 0.843 - ale.lives: 2.006\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 2.6949\n",
      "9 episodes - episode_reward: 2720.778 [1776.000, 4138.000] - loss: 474.555 - mean_absolute_error: 65.815 - mean_q: 75.747 - mean_eps: 0.834 - ale.lives: 2.044\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.9337\n",
      "11 episodes - episode_reward: 2128.000 [364.000, 4507.000] - loss: 527.047 - mean_absolute_error: 70.786 - mean_q: 81.153 - mean_eps: 0.825 - ale.lives: 2.057\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.2188\n",
      "12 episodes - episode_reward: 1867.667 [295.000, 4575.000] - loss: 483.542 - mean_absolute_error: 70.597 - mean_q: 80.244 - mean_eps: 0.816 - ale.lives: 2.014\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.5552\n",
      "10 episodes - episode_reward: 2496.700 [343.000, 4819.000] - loss: 531.992 - mean_absolute_error: 72.791 - mean_q: 82.651 - mean_eps: 0.807 - ale.lives: 2.175\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.3517\n",
      "11 episodes - episode_reward: 1991.727 [551.000, 3694.000] - loss: 525.676 - mean_absolute_error: 75.008 - mean_q: 84.954 - mean_eps: 0.798 - ale.lives: 2.020\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.8291\n",
      "9 episodes - episode_reward: 2825.444 [1399.000, 4559.000] - loss: 576.761 - mean_absolute_error: 76.325 - mean_q: 87.009 - mean_eps: 0.789 - ale.lives: 2.062\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.0295\n",
      "10 episodes - episode_reward: 2525.800 [444.000, 5157.000] - loss: 644.804 - mean_absolute_error: 77.356 - mean_q: 87.962 - mean_eps: 0.780 - ale.lives: 2.021\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 3.0405\n",
      "8 episodes - episode_reward: 3579.875 [1617.000, 6995.000] - loss: 753.090 - mean_absolute_error: 85.871 - mean_q: 97.529 - mean_eps: 0.771 - ale.lives: 1.941\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 3.0496\n",
      "9 episodes - episode_reward: 3561.444 [928.000, 9694.000] - loss: 708.564 - mean_absolute_error: 91.565 - mean_q: 103.393 - mean_eps: 0.762 - ale.lives: 2.077\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.6756\n",
      "11 episodes - episode_reward: 2460.727 [1054.000, 5250.000] - loss: 695.675 - mean_absolute_error: 94.761 - mean_q: 107.482 - mean_eps: 0.753 - ale.lives: 2.002\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.4300\n",
      "10 episodes - episode_reward: 2429.900 [762.000, 4157.000] - loss: 590.754 - mean_absolute_error: 94.839 - mean_q: 106.183 - mean_eps: 0.744 - ale.lives: 2.073\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 1.9183\n",
      "done, took 1751.365 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn2.fit(env2, nb_steps=300000, visualize=False, verbose=1) #callbacks=[save_at_episode()]\n",
    "dqn2.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model2_weights',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "u9J6XulxKIpA",
    "outputId": "903ad9d1-f025-444a-a5e5-036a2e4018d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 5328.000, steps: 1644\n",
      "Episode 2: reward: 2230.000, steps: 973\n",
      "Episode 3: reward: 7591.000, steps: 1193\n",
      "Episode 4: reward: 3549.000, steps: 1038\n",
      "Episode 5: reward: 3255.000, steps: 1012\n",
      "Episode 6: reward: 3545.000, steps: 1313\n",
      "Episode 7: reward: 3600.000, steps: 1102\n",
      "Episode 8: reward: 10527.000, steps: 1932\n",
      "Episode 9: reward: 5032.000, steps: 1081\n",
      "Episode 10: reward: 4716.000, steps: 1471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fddd53570f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn2.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model2_weights')\n",
    "dqn2.test(env2, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V8unQng14c_H",
    "outputId": "16a26633-1651-4911-9a07-583f3c43ec7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shared_weights, shared_bias = model1.get_layer('shared_layer').weights\n",
    "sns.distplot(K.eval(shared_weights).ravel())\n",
    "plt.savefig('/content/drive/My Drive/Colab Notebooks/Game Data/shared_weights_after.jpg')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKc5-MeI6Map"
   },
   "outputs": [],
   "source": [
    "dqn1.save_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_changed',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "Tr4tIAnt6WCD",
    "outputId": "c6aa4fe2-04a0-42f9-8551-3de900bde0d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 300.000, steps: 253\n",
      "Episode 2: reward: 125.000, steps: 208\n",
      "Episode 3: reward: 300.000, steps: 253\n",
      "Episode 4: reward: 300.000, steps: 245\n",
      "Episode 5: reward: 175.000, steps: 195\n",
      "Episode 6: reward: 300.000, steps: 251\n",
      "Episode 7: reward: 300.000, steps: 249\n",
      "Episode 8: reward: 175.000, steps: 197\n",
      "Episode 9: reward: 300.000, steps: 244\n",
      "Episode 10: reward: 300.000, steps: 245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fddd5357550>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn1.load_weights('/content/drive/My Drive/Colab Notebooks/Game Data/DQN_model1_weights_changed')\n",
    "dqn1.test(env1, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4nuVUQWWOiw"
   },
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKtA_BmjIZSF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SharedArchitecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
